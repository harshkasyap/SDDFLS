{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class Parser:\n",
    "    \"\"\"Parameters for training\"\"\"\n",
    "    def __init__(self):\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.001\n",
    "        self.test_batch_size = 8\n",
    "        self.batch_size = 8\n",
    "        self.log_interval = 10\n",
    "        self.seed = 1\n",
    "    \n",
    "args = Parser()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/BostonHousing/boston_housing.pickle','rb') as f:\n",
    "    ((X, y), (X_test, y_test)) = pickle.load(f)\n",
    "\n",
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "# preprocessing\n",
    "mean = X.mean(0, keepdim=True)\n",
    "dev = X.std(0, keepdim=True)\n",
    "mean[:, 3] = 0. # the feature at column 3 is binary,\n",
    "dev[:, 3] = 1.  # so we don't standardize it\n",
    "X = (X - mean) / dev\n",
    "X_test = (X_test - mean) / dev\n",
    "train = TensorDataset(X, y)\n",
    "test = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(test, batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(13, 32)\n",
    "        self.fc2 = nn.Linear(32, 24)\n",
    "        self.fc3 = nn.Linear(24, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 13)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/opt/conda/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.2.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tf_encrypted/session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "\n",
    "hook = sy.TorchHook(torch)\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "james = sy.VirtualWorker(hook, id=\"james\")\n",
    "\n",
    "compute_nodes = [bob, alice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distributed_dataset = []\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    #print(batch_idx)\n",
    "    data = data.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    target = target.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    train_distributed_dataset.append((data, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data,target) in enumerate(train_distributed_dataset):\n",
    "        worker = data.location\n",
    "        model.send(worker)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # update the model\n",
    "        pred = model(data)\n",
    "        loss = F.mse_loss(pred.view(-1), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get()\n",
    "            \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * data.shape[0], len(train_loader),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += F.mse_loss(output.view(-1), target, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/51 (0%)]\tLoss: 7.060624\n",
      "Train Epoch: 1 [80/51 (20%)]\tLoss: 4.177224\n",
      "Train Epoch: 1 [160/51 (39%)]\tLoss: 3.379756\n",
      "Train Epoch: 1 [240/51 (59%)]\tLoss: 3.991245\n",
      "Train Epoch: 1 [320/51 (78%)]\tLoss: 7.824611\n",
      "Train Epoch: 1 [200/51 (98%)]\tLoss: 5.528558\n",
      "Train Epoch: 2 [0/51 (0%)]\tLoss: 6.994240\n",
      "Train Epoch: 2 [80/51 (20%)]\tLoss: 3.917075\n",
      "Train Epoch: 2 [160/51 (39%)]\tLoss: 3.480738\n",
      "Train Epoch: 2 [240/51 (59%)]\tLoss: 3.827816\n",
      "Train Epoch: 2 [320/51 (78%)]\tLoss: 7.459150\n",
      "Train Epoch: 2 [200/51 (98%)]\tLoss: 5.203971\n",
      "Train Epoch: 3 [0/51 (0%)]\tLoss: 7.004023\n",
      "Train Epoch: 3 [80/51 (20%)]\tLoss: 3.830621\n",
      "Train Epoch: 3 [160/51 (39%)]\tLoss: 3.592762\n",
      "Train Epoch: 3 [240/51 (59%)]\tLoss: 3.695036\n",
      "Train Epoch: 3 [320/51 (78%)]\tLoss: 7.138633\n",
      "Train Epoch: 3 [200/51 (98%)]\tLoss: 4.909476\n",
      "Train Epoch: 4 [0/51 (0%)]\tLoss: 6.751232\n",
      "Train Epoch: 4 [80/51 (20%)]\tLoss: 3.755028\n",
      "Train Epoch: 4 [160/51 (39%)]\tLoss: 3.508644\n",
      "Train Epoch: 4 [240/51 (59%)]\tLoss: 3.506132\n",
      "Train Epoch: 4 [320/51 (78%)]\tLoss: 6.981020\n",
      "Train Epoch: 4 [200/51 (98%)]\tLoss: 4.802821\n",
      "Train Epoch: 5 [0/51 (0%)]\tLoss: 6.684203\n",
      "Train Epoch: 5 [80/51 (20%)]\tLoss: 3.727417\n",
      "Train Epoch: 5 [160/51 (39%)]\tLoss: 3.472540\n",
      "Train Epoch: 5 [240/51 (59%)]\tLoss: 3.365635\n",
      "Train Epoch: 5 [320/51 (78%)]\tLoss: 6.904458\n",
      "Train Epoch: 5 [200/51 (98%)]\tLoss: 4.644075\n",
      "Train Epoch: 6 [0/51 (0%)]\tLoss: 6.642040\n",
      "Train Epoch: 6 [80/51 (20%)]\tLoss: 3.664208\n",
      "Train Epoch: 6 [160/51 (39%)]\tLoss: 3.438784\n",
      "Train Epoch: 6 [240/51 (59%)]\tLoss: 3.202746\n",
      "Train Epoch: 6 [320/51 (78%)]\tLoss: 6.829812\n",
      "Train Epoch: 6 [200/51 (98%)]\tLoss: 4.522412\n",
      "Train Epoch: 7 [0/51 (0%)]\tLoss: 6.565149\n",
      "Train Epoch: 7 [80/51 (20%)]\tLoss: 3.570517\n",
      "Train Epoch: 7 [160/51 (39%)]\tLoss: 3.416960\n",
      "Train Epoch: 7 [240/51 (59%)]\tLoss: 3.113039\n",
      "Train Epoch: 7 [320/51 (78%)]\tLoss: 6.674186\n",
      "Train Epoch: 7 [200/51 (98%)]\tLoss: 4.232191\n",
      "Train Epoch: 8 [0/51 (0%)]\tLoss: 6.426906\n",
      "Train Epoch: 8 [80/51 (20%)]\tLoss: 3.510462\n",
      "Train Epoch: 8 [160/51 (39%)]\tLoss: 3.476645\n",
      "Train Epoch: 8 [240/51 (59%)]\tLoss: 3.010004\n",
      "Train Epoch: 8 [320/51 (78%)]\tLoss: 6.520691\n",
      "Train Epoch: 8 [200/51 (98%)]\tLoss: 4.046004\n",
      "Train Epoch: 9 [0/51 (0%)]\tLoss: 6.387328\n",
      "Train Epoch: 9 [80/51 (20%)]\tLoss: 3.495404\n",
      "Train Epoch: 9 [160/51 (39%)]\tLoss: 3.466648\n",
      "Train Epoch: 9 [240/51 (59%)]\tLoss: 2.923546\n",
      "Train Epoch: 9 [320/51 (78%)]\tLoss: 6.478467\n",
      "Train Epoch: 9 [200/51 (98%)]\tLoss: 3.904519\n",
      "Train Epoch: 10 [0/51 (0%)]\tLoss: 6.178332\n",
      "Train Epoch: 10 [80/51 (20%)]\tLoss: 3.397410\n",
      "Train Epoch: 10 [160/51 (39%)]\tLoss: 3.448346\n",
      "Train Epoch: 10 [240/51 (59%)]\tLoss: 2.853581\n",
      "Train Epoch: 10 [320/51 (78%)]\tLoss: 6.484494\n",
      "Train Epoch: 10 [200/51 (98%)]\tLoss: 3.680728\n",
      "Total 16.53 s\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "\n",
    "    \n",
    "total_time = time.time() - t\n",
    "print('Total', round(total_time, 2), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 22.5260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_dataset = (list(),list())\n",
    "\n",
    "train_distributed_dataset = []\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    data = data.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    target = target.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    remote_dataset[batch_idx % len(compute_nodes)].append((data, target))\n",
    "\n",
    "def update(data, target, model, optimizer):\n",
    "    model.send(data.location)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(data)\n",
    "    loss = F.mse_loss(pred.view(-1), target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return model\n",
    "\n",
    "bobs_model = Net()\n",
    "alices_model = Net()\n",
    "\n",
    "bobs_optimizer = optim.SGD(bobs_model.parameters(), lr=args.lr)\n",
    "alices_optimizer = optim.SGD(alices_model.parameters(), lr=args.lr)\n",
    "\n",
    "models = [bobs_model, alices_model]\n",
    "params = [list(bobs_model.parameters()), list(alices_model.parameters())]\n",
    "optimizers = [bobs_optimizer, alices_optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is selecting which batch to train on\n",
    "data_index = 0\n",
    "# update remote models\n",
    "# we could iterate this multiple times before proceeding, but we're only iterating once per worker here\n",
    "for remote_index in range(len(compute_nodes)):\n",
    "    data, target = remote_dataset[remote_index][data_index]\n",
    "    models[remote_index] = update(data, target, models[remote_index], optimizers[remote_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list where we'll deposit our encrypted model average\n",
    "new_params = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each parameter\n",
    "for param_i in range(len(params[0])):\n",
    "\n",
    "    # for each worker\n",
    "    spdz_params = list()\n",
    "    for remote_index in range(len(compute_nodes)):\n",
    "        \n",
    "        # select the identical parameter from each worker and copy it\n",
    "        copy_of_parameter = params[remote_index][param_i].copy()\n",
    "        \n",
    "        # since SMPC can only work with integers (not floats), we need\n",
    "        # to use Integers to store decimal information. In other words,\n",
    "        # we need to use \"Fixed Precision\" encoding.\n",
    "        fixed_precision_param = copy_of_parameter.fix_precision()\n",
    "        \n",
    "        # now we encrypt it on the remote machine. Note that \n",
    "        # fixed_precision_param is ALREADY a pointer. Thus, when\n",
    "        # we call share, it actually encrypts the data that the\n",
    "        # data is pointing TO. This returns a POINTER to the \n",
    "        # MPC secret shared object, which we need to fetch.\n",
    "        encrypted_param = fixed_precision_param.share(bob, alice, crypto_provider=james)\n",
    "        \n",
    "        # now we fetch the pointer to the MPC shared value\n",
    "        param = encrypted_param.get()\n",
    "        \n",
    "        # save the parameter so we can average it with the same parameter\n",
    "        # from the other workers\n",
    "        spdz_params.append(param)\n",
    "\n",
    "    # average params from multiple workers, fetch them to the local machine\n",
    "    # decrypt and decode (from fixed precision) back into a floating point number\n",
    "    new_param = (spdz_params[0] + spdz_params[1]).get().float_precision()/2\n",
    "    \n",
    "    # save the new averaged parameter\n",
    "    new_params.append(new_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for model in params:\n",
    "        for param in model:\n",
    "            param *= 0\n",
    "\n",
    "    for model in models:\n",
    "        model.get()\n",
    "\n",
    "    for remote_index in range(len(compute_nodes)):\n",
    "        for param_index in range(len(params[remote_index])):\n",
    "            params[remote_index][param_index].set_(new_params[param_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    for data_index in range(len(remote_dataset[0])-1):\n",
    "        # update remote models\n",
    "        for remote_index in range(len(compute_nodes)):\n",
    "            data, target = remote_dataset[remote_index][data_index]\n",
    "            models[remote_index] = update(data, target, models[remote_index], optimizers[remote_index])\n",
    "\n",
    "        # encrypted aggregation\n",
    "        new_params = list()\n",
    "        for param_i in range(len(params[0])):\n",
    "            spdz_params = list()\n",
    "            for remote_index in range(len(compute_nodes)):\n",
    "                spdz_params.append(params[remote_index][param_i].copy().fix_precision().share(bob, alice, crypto_provider=james).get())\n",
    "\n",
    "            new_param = (spdz_params[0] + spdz_params[1]).get().float_precision()/2\n",
    "            new_params.append(new_param)\n",
    "\n",
    "        # cleanup\n",
    "        with torch.no_grad():\n",
    "            for model in params:\n",
    "                for param in model:\n",
    "                    param *= 0\n",
    "\n",
    "            for model in models:\n",
    "                model.get()\n",
    "\n",
    "            for remote_index in range(len(compute_nodes)):\n",
    "                for param_index in range(len(params[remote_index])):\n",
    "                    params[remote_index][param_index].set_(new_params[param_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    models[0].eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        output = models[0](data)\n",
    "        test_loss += F.mse_loss(output.view(-1), target, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}\\n'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Test set: Average loss: 482.6557\n",
      "\n",
      "Epoch 2\n",
      "Test set: Average loss: 39.8017\n",
      "\n",
      "Epoch 3\n",
      "Test set: Average loss: 22.6465\n",
      "\n",
      "Epoch 4\n",
      "Test set: Average loss: 20.5664\n",
      "\n",
      "Epoch 5\n",
      "Test set: Average loss: 19.4792\n",
      "\n",
      "Epoch 6\n",
      "Test set: Average loss: 18.8669\n",
      "\n",
      "Epoch 7\n",
      "Test set: Average loss: 18.5057\n",
      "\n",
      "Epoch 8\n",
      "Test set: Average loss: 18.4037\n",
      "\n",
      "Epoch 9\n",
      "Test set: Average loss: 18.3137\n",
      "\n",
      "Epoch 10\n",
      "Test set: Average loss: 18.1742\n",
      "\n",
      "Total 36.7 s\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    train(epoch)\n",
    "    test()\n",
    "\n",
    "    \n",
    "total_time = time.time() - t\n",
    "print('Total', round(total_time, 2), 's')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
