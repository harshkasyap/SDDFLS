{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "import syft as sy  # <-- NEW: import the Pysyft library\n",
    "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")  # <-- NEW: define remote worker bob\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")  # <-- NEW: and alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 30\n",
    "        self.save_model = False\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "    .federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, federated_train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
    "        model.send(data.location) # <-- NEW: send the model to the right location\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get() # <-- NEW: get the model back\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get() # <-- NEW: get the loss back\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(federated_train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 ms, sys: 0 ns, total: 10 ms\n",
      "Wall time: 13.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.281243\n",
      "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 2.213646\n",
      "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 2.076110\n",
      "Train Epoch: 1 [5760/60032 (10%)]\tLoss: 1.688049\n",
      "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 1.192953\n",
      "Train Epoch: 1 [9600/60032 (16%)]\tLoss: 0.749067\n",
      "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 0.450151\n",
      "Train Epoch: 1 [13440/60032 (22%)]\tLoss: 0.463166\n",
      "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 0.266511\n",
      "Train Epoch: 1 [17280/60032 (29%)]\tLoss: 0.504612\n",
      "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.637299\n",
      "Train Epoch: 1 [21120/60032 (35%)]\tLoss: 0.451403\n",
      "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 0.289505\n",
      "Train Epoch: 1 [24960/60032 (42%)]\tLoss: 0.332079\n",
      "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.424554\n",
      "Train Epoch: 1 [28800/60032 (48%)]\tLoss: 0.551205\n",
      "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 0.454467\n",
      "Train Epoch: 1 [32640/60032 (54%)]\tLoss: 0.256387\n",
      "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.289773\n",
      "Train Epoch: 1 [36480/60032 (61%)]\tLoss: 0.416787\n",
      "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.217063\n",
      "Train Epoch: 1 [40320/60032 (67%)]\tLoss: 0.269475\n",
      "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.167541\n",
      "Train Epoch: 1 [44160/60032 (74%)]\tLoss: 0.205039\n",
      "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.187578\n",
      "Train Epoch: 1 [48000/60032 (80%)]\tLoss: 0.208620\n",
      "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.193658\n",
      "Train Epoch: 1 [51840/60032 (86%)]\tLoss: 0.237273\n",
      "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.069679\n",
      "Train Epoch: 1 [55680/60032 (93%)]\tLoss: 0.193635\n",
      "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.151353\n",
      "Train Epoch: 1 [59520/60032 (99%)]\tLoss: 0.203356\n",
      "\n",
      "Test set: Average loss: 0.1644, Accuracy: 9507/10000 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.259791\n",
      "Train Epoch: 2 [1920/60032 (3%)]\tLoss: 0.150967\n",
      "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.185301\n",
      "Train Epoch: 2 [5760/60032 (10%)]\tLoss: 0.159885\n",
      "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.128158\n",
      "Train Epoch: 2 [9600/60032 (16%)]\tLoss: 0.216327\n",
      "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.419687\n",
      "Train Epoch: 2 [13440/60032 (22%)]\tLoss: 0.161093\n",
      "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.133004\n",
      "Train Epoch: 2 [17280/60032 (29%)]\tLoss: 0.102430\n",
      "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.419970\n",
      "Train Epoch: 2 [21120/60032 (35%)]\tLoss: 0.240581\n",
      "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.100725\n",
      "Train Epoch: 2 [24960/60032 (42%)]\tLoss: 0.073096\n",
      "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.041025\n",
      "Train Epoch: 2 [28800/60032 (48%)]\tLoss: 0.147581\n",
      "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.099320\n",
      "Train Epoch: 2 [32640/60032 (54%)]\tLoss: 0.187103\n",
      "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.145348\n",
      "Train Epoch: 2 [36480/60032 (61%)]\tLoss: 0.120465\n",
      "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.189215\n",
      "Train Epoch: 2 [40320/60032 (67%)]\tLoss: 0.067905\n",
      "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.124002\n",
      "Train Epoch: 2 [44160/60032 (74%)]\tLoss: 0.150078\n",
      "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.140865\n",
      "Train Epoch: 2 [48000/60032 (80%)]\tLoss: 0.067519\n",
      "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.234784\n",
      "Train Epoch: 2 [51840/60032 (86%)]\tLoss: 0.210550\n",
      "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.051918\n",
      "Train Epoch: 2 [55680/60032 (93%)]\tLoss: 0.163640\n",
      "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.052758\n",
      "Train Epoch: 2 [59520/60032 (99%)]\tLoss: 0.080870\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.064604\n",
      "Train Epoch: 3 [1920/60032 (3%)]\tLoss: 0.161520\n",
      "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.059605\n",
      "Train Epoch: 3 [5760/60032 (10%)]\tLoss: 0.027678\n",
      "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.154044\n",
      "Train Epoch: 3 [9600/60032 (16%)]\tLoss: 0.083626\n",
      "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.078629\n",
      "Train Epoch: 3 [13440/60032 (22%)]\tLoss: 0.246456\n",
      "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.112198\n",
      "Train Epoch: 3 [17280/60032 (29%)]\tLoss: 0.081796\n",
      "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.181025\n",
      "Train Epoch: 3 [21120/60032 (35%)]\tLoss: 0.030960\n",
      "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.075645\n",
      "Train Epoch: 3 [24960/60032 (42%)]\tLoss: 0.046067\n",
      "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.080112\n",
      "Train Epoch: 3 [28800/60032 (48%)]\tLoss: 0.043483\n",
      "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.102731\n",
      "Train Epoch: 3 [32640/60032 (54%)]\tLoss: 0.032424\n",
      "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.193488\n",
      "Train Epoch: 3 [36480/60032 (61%)]\tLoss: 0.039252\n",
      "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.154429\n",
      "Train Epoch: 3 [40320/60032 (67%)]\tLoss: 0.070648\n",
      "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.146958\n",
      "Train Epoch: 3 [44160/60032 (74%)]\tLoss: 0.098454\n",
      "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.049630\n",
      "Train Epoch: 3 [48000/60032 (80%)]\tLoss: 0.028858\n",
      "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.025729\n",
      "Train Epoch: 3 [51840/60032 (86%)]\tLoss: 0.021829\n",
      "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.090815\n",
      "Train Epoch: 3 [55680/60032 (93%)]\tLoss: 0.094911\n",
      "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.055689\n",
      "Train Epoch: 3 [59520/60032 (99%)]\tLoss: 0.044861\n",
      "\n",
      "Test set: Average loss: 0.0718, Accuracy: 9773/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.035018\n",
      "Train Epoch: 4 [1920/60032 (3%)]\tLoss: 0.016889\n",
      "Train Epoch: 4 [3840/60032 (6%)]\tLoss: 0.103755\n",
      "Train Epoch: 4 [5760/60032 (10%)]\tLoss: 0.045007\n",
      "Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.023247\n",
      "Train Epoch: 4 [9600/60032 (16%)]\tLoss: 0.206280\n",
      "Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.091859\n",
      "Train Epoch: 4 [13440/60032 (22%)]\tLoss: 0.050225\n",
      "Train Epoch: 4 [15360/60032 (26%)]\tLoss: 0.045448\n",
      "Train Epoch: 4 [17280/60032 (29%)]\tLoss: 0.166518\n",
      "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.015249\n",
      "Train Epoch: 4 [21120/60032 (35%)]\tLoss: 0.153238\n",
      "Train Epoch: 4 [23040/60032 (38%)]\tLoss: 0.113355\n",
      "Train Epoch: 4 [24960/60032 (42%)]\tLoss: 0.050735\n",
      "Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.097562\n",
      "Train Epoch: 4 [28800/60032 (48%)]\tLoss: 0.017901\n",
      "Train Epoch: 4 [30720/60032 (51%)]\tLoss: 0.014326\n",
      "Train Epoch: 4 [32640/60032 (54%)]\tLoss: 0.072016\n",
      "Train Epoch: 4 [34560/60032 (58%)]\tLoss: 0.079549\n",
      "Train Epoch: 4 [36480/60032 (61%)]\tLoss: 0.092428\n",
      "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.015654\n",
      "Train Epoch: 4 [40320/60032 (67%)]\tLoss: 0.075396\n",
      "Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.051163\n",
      "Train Epoch: 4 [44160/60032 (74%)]\tLoss: 0.416114\n",
      "Train Epoch: 4 [46080/60032 (77%)]\tLoss: 0.079205\n",
      "Train Epoch: 4 [48000/60032 (80%)]\tLoss: 0.093206\n",
      "Train Epoch: 4 [49920/60032 (83%)]\tLoss: 0.107088\n",
      "Train Epoch: 4 [51840/60032 (86%)]\tLoss: 0.057873\n",
      "Train Epoch: 4 [53760/60032 (90%)]\tLoss: 0.068619\n",
      "Train Epoch: 4 [55680/60032 (93%)]\tLoss: 0.075827\n",
      "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.039569\n",
      "Train Epoch: 4 [59520/60032 (99%)]\tLoss: 0.013840\n",
      "\n",
      "Test set: Average loss: 0.0580, Accuracy: 9830/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.061200\n",
      "Train Epoch: 5 [1920/60032 (3%)]\tLoss: 0.085283\n",
      "Train Epoch: 5 [3840/60032 (6%)]\tLoss: 0.036073\n",
      "Train Epoch: 5 [5760/60032 (10%)]\tLoss: 0.040289\n",
      "Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.024395\n",
      "Train Epoch: 5 [9600/60032 (16%)]\tLoss: 0.032838\n",
      "Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.062529\n",
      "Train Epoch: 5 [13440/60032 (22%)]\tLoss: 0.058253\n",
      "Train Epoch: 5 [15360/60032 (26%)]\tLoss: 0.011053\n",
      "Train Epoch: 5 [17280/60032 (29%)]\tLoss: 0.053004\n",
      "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.030161\n",
      "Train Epoch: 5 [21120/60032 (35%)]\tLoss: 0.133277\n",
      "Train Epoch: 5 [23040/60032 (38%)]\tLoss: 0.154559\n",
      "Train Epoch: 5 [24960/60032 (42%)]\tLoss: 0.033700\n",
      "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.041407\n",
      "Train Epoch: 5 [28800/60032 (48%)]\tLoss: 0.045934\n",
      "Train Epoch: 5 [30720/60032 (51%)]\tLoss: 0.098533\n",
      "Train Epoch: 5 [32640/60032 (54%)]\tLoss: 0.047038\n",
      "Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.015488\n",
      "Train Epoch: 5 [36480/60032 (61%)]\tLoss: 0.136264\n",
      "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.030557\n",
      "Train Epoch: 5 [40320/60032 (67%)]\tLoss: 0.032627\n",
      "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.029956\n",
      "Train Epoch: 5 [44160/60032 (74%)]\tLoss: 0.018015\n",
      "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.025357\n",
      "Train Epoch: 5 [48000/60032 (80%)]\tLoss: 0.024660\n",
      "Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.019768\n",
      "Train Epoch: 5 [51840/60032 (86%)]\tLoss: 0.017934\n",
      "Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.034111\n",
      "Train Epoch: 5 [55680/60032 (93%)]\tLoss: 0.017011\n",
      "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.080442\n",
      "Train Epoch: 5 [59520/60032 (99%)]\tLoss: 0.037619\n",
      "\n",
      "Test set: Average loss: 0.0494, Accuracy: 9849/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.085920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [1920/60032 (3%)]\tLoss: 0.027932\n",
      "Train Epoch: 6 [3840/60032 (6%)]\tLoss: 0.058142\n",
      "Train Epoch: 6 [5760/60032 (10%)]\tLoss: 0.046974\n",
      "Train Epoch: 6 [7680/60032 (13%)]\tLoss: 0.010254\n",
      "Train Epoch: 6 [9600/60032 (16%)]\tLoss: 0.070102\n",
      "Train Epoch: 6 [11520/60032 (19%)]\tLoss: 0.018006\n",
      "Train Epoch: 6 [13440/60032 (22%)]\tLoss: 0.057572\n",
      "Train Epoch: 6 [15360/60032 (26%)]\tLoss: 0.024211\n",
      "Train Epoch: 6 [17280/60032 (29%)]\tLoss: 0.013318\n",
      "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.143585\n",
      "Train Epoch: 6 [21120/60032 (35%)]\tLoss: 0.073294\n",
      "Train Epoch: 6 [23040/60032 (38%)]\tLoss: 0.028297\n",
      "Train Epoch: 6 [24960/60032 (42%)]\tLoss: 0.058085\n",
      "Train Epoch: 6 [26880/60032 (45%)]\tLoss: 0.035323\n",
      "Train Epoch: 6 [28800/60032 (48%)]\tLoss: 0.029896\n",
      "Train Epoch: 6 [30720/60032 (51%)]\tLoss: 0.024490\n",
      "Train Epoch: 6 [32640/60032 (54%)]\tLoss: 0.117935\n",
      "Train Epoch: 6 [34560/60032 (58%)]\tLoss: 0.016368\n",
      "Train Epoch: 6 [36480/60032 (61%)]\tLoss: 0.035915\n",
      "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.082854\n",
      "Train Epoch: 6 [40320/60032 (67%)]\tLoss: 0.033375\n",
      "Train Epoch: 6 [42240/60032 (70%)]\tLoss: 0.007192\n",
      "Train Epoch: 6 [44160/60032 (74%)]\tLoss: 0.103974\n",
      "Train Epoch: 6 [46080/60032 (77%)]\tLoss: 0.099437\n",
      "Train Epoch: 6 [48000/60032 (80%)]\tLoss: 0.027409\n",
      "Train Epoch: 6 [49920/60032 (83%)]\tLoss: 0.054119\n",
      "Train Epoch: 6 [51840/60032 (86%)]\tLoss: 0.008870\n",
      "Train Epoch: 6 [53760/60032 (90%)]\tLoss: 0.009245\n",
      "Train Epoch: 6 [55680/60032 (93%)]\tLoss: 0.073352\n",
      "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.022670\n",
      "Train Epoch: 6 [59520/60032 (99%)]\tLoss: 0.041855\n",
      "\n",
      "Test set: Average loss: 0.0468, Accuracy: 9846/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.015377\n",
      "Train Epoch: 7 [1920/60032 (3%)]\tLoss: 0.014008\n",
      "Train Epoch: 7 [3840/60032 (6%)]\tLoss: 0.064969\n",
      "Train Epoch: 7 [5760/60032 (10%)]\tLoss: 0.028255\n",
      "Train Epoch: 7 [7680/60032 (13%)]\tLoss: 0.125676\n",
      "Train Epoch: 7 [9600/60032 (16%)]\tLoss: 0.030826\n",
      "Train Epoch: 7 [11520/60032 (19%)]\tLoss: 0.005265\n",
      "Train Epoch: 7 [13440/60032 (22%)]\tLoss: 0.035185\n",
      "Train Epoch: 7 [15360/60032 (26%)]\tLoss: 0.109513\n",
      "Train Epoch: 7 [17280/60032 (29%)]\tLoss: 0.054594\n",
      "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.010482\n",
      "Train Epoch: 7 [21120/60032 (35%)]\tLoss: 0.019319\n",
      "Train Epoch: 7 [23040/60032 (38%)]\tLoss: 0.076855\n",
      "Train Epoch: 7 [24960/60032 (42%)]\tLoss: 0.023325\n",
      "Train Epoch: 7 [26880/60032 (45%)]\tLoss: 0.049116\n",
      "Train Epoch: 7 [28800/60032 (48%)]\tLoss: 0.122414\n",
      "Train Epoch: 7 [30720/60032 (51%)]\tLoss: 0.023429\n",
      "Train Epoch: 7 [32640/60032 (54%)]\tLoss: 0.045182\n",
      "Train Epoch: 7 [34560/60032 (58%)]\tLoss: 0.059088\n",
      "Train Epoch: 7 [36480/60032 (61%)]\tLoss: 0.088957\n",
      "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.065294\n",
      "Train Epoch: 7 [40320/60032 (67%)]\tLoss: 0.032919\n",
      "Train Epoch: 7 [42240/60032 (70%)]\tLoss: 0.046742\n",
      "Train Epoch: 7 [44160/60032 (74%)]\tLoss: 0.045909\n",
      "Train Epoch: 7 [46080/60032 (77%)]\tLoss: 0.067735\n",
      "Train Epoch: 7 [48000/60032 (80%)]\tLoss: 0.007076\n",
      "Train Epoch: 7 [49920/60032 (83%)]\tLoss: 0.032870\n",
      "Train Epoch: 7 [51840/60032 (86%)]\tLoss: 0.094082\n",
      "Train Epoch: 7 [53760/60032 (90%)]\tLoss: 0.030432\n",
      "Train Epoch: 7 [55680/60032 (93%)]\tLoss: 0.011305\n",
      "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.006878\n",
      "Train Epoch: 7 [59520/60032 (99%)]\tLoss: 0.054680\n",
      "\n",
      "Test set: Average loss: 0.0490, Accuracy: 9840/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60032 (0%)]\tLoss: 0.008807\n",
      "Train Epoch: 8 [1920/60032 (3%)]\tLoss: 0.017013\n",
      "Train Epoch: 8 [3840/60032 (6%)]\tLoss: 0.053860\n",
      "Train Epoch: 8 [5760/60032 (10%)]\tLoss: 0.141484\n",
      "Train Epoch: 8 [7680/60032 (13%)]\tLoss: 0.023958\n",
      "Train Epoch: 8 [9600/60032 (16%)]\tLoss: 0.028829\n",
      "Train Epoch: 8 [11520/60032 (19%)]\tLoss: 0.099621\n",
      "Train Epoch: 8 [13440/60032 (22%)]\tLoss: 0.011643\n",
      "Train Epoch: 8 [15360/60032 (26%)]\tLoss: 0.014405\n",
      "Train Epoch: 8 [17280/60032 (29%)]\tLoss: 0.008784\n",
      "Train Epoch: 8 [19200/60032 (32%)]\tLoss: 0.011941\n",
      "Train Epoch: 8 [21120/60032 (35%)]\tLoss: 0.004948\n",
      "Train Epoch: 8 [23040/60032 (38%)]\tLoss: 0.125748\n",
      "Train Epoch: 8 [24960/60032 (42%)]\tLoss: 0.071298\n",
      "Train Epoch: 8 [26880/60032 (45%)]\tLoss: 0.073004\n",
      "Train Epoch: 8 [28800/60032 (48%)]\tLoss: 0.036548\n",
      "Train Epoch: 8 [30720/60032 (51%)]\tLoss: 0.011905\n",
      "Train Epoch: 8 [32640/60032 (54%)]\tLoss: 0.035063\n",
      "Train Epoch: 8 [34560/60032 (58%)]\tLoss: 0.055130\n",
      "Train Epoch: 8 [36480/60032 (61%)]\tLoss: 0.064708\n",
      "Train Epoch: 8 [38400/60032 (64%)]\tLoss: 0.060315\n",
      "Train Epoch: 8 [40320/60032 (67%)]\tLoss: 0.058542\n",
      "Train Epoch: 8 [42240/60032 (70%)]\tLoss: 0.086427\n",
      "Train Epoch: 8 [44160/60032 (74%)]\tLoss: 0.056279\n",
      "Train Epoch: 8 [46080/60032 (77%)]\tLoss: 0.009393\n",
      "Train Epoch: 8 [48000/60032 (80%)]\tLoss: 0.004192\n",
      "Train Epoch: 8 [49920/60032 (83%)]\tLoss: 0.017954\n",
      "Train Epoch: 8 [51840/60032 (86%)]\tLoss: 0.196410\n",
      "Train Epoch: 8 [53760/60032 (90%)]\tLoss: 0.028031\n",
      "Train Epoch: 8 [55680/60032 (93%)]\tLoss: 0.027476\n",
      "Train Epoch: 8 [57600/60032 (96%)]\tLoss: 0.014247\n",
      "Train Epoch: 8 [59520/60032 (99%)]\tLoss: 0.069068\n",
      "\n",
      "Test set: Average loss: 0.0391, Accuracy: 9869/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60032 (0%)]\tLoss: 0.134494\n",
      "Train Epoch: 9 [1920/60032 (3%)]\tLoss: 0.023576\n",
      "Train Epoch: 9 [3840/60032 (6%)]\tLoss: 0.025586\n",
      "Train Epoch: 9 [5760/60032 (10%)]\tLoss: 0.106019\n",
      "Train Epoch: 9 [7680/60032 (13%)]\tLoss: 0.003274\n",
      "Train Epoch: 9 [9600/60032 (16%)]\tLoss: 0.023490\n",
      "Train Epoch: 9 [11520/60032 (19%)]\tLoss: 0.105883\n",
      "Train Epoch: 9 [13440/60032 (22%)]\tLoss: 0.016266\n",
      "Train Epoch: 9 [15360/60032 (26%)]\tLoss: 0.009955\n",
      "Train Epoch: 9 [17280/60032 (29%)]\tLoss: 0.010105\n",
      "Train Epoch: 9 [19200/60032 (32%)]\tLoss: 0.052723\n",
      "Train Epoch: 9 [21120/60032 (35%)]\tLoss: 0.021687\n",
      "Train Epoch: 9 [23040/60032 (38%)]\tLoss: 0.003312\n",
      "Train Epoch: 9 [24960/60032 (42%)]\tLoss: 0.023049\n",
      "Train Epoch: 9 [26880/60032 (45%)]\tLoss: 0.029292\n",
      "Train Epoch: 9 [28800/60032 (48%)]\tLoss: 0.013759\n",
      "Train Epoch: 9 [30720/60032 (51%)]\tLoss: 0.085150\n",
      "Train Epoch: 9 [32640/60032 (54%)]\tLoss: 0.068231\n",
      "Train Epoch: 9 [34560/60032 (58%)]\tLoss: 0.007805\n",
      "Train Epoch: 9 [36480/60032 (61%)]\tLoss: 0.006951\n",
      "Train Epoch: 9 [38400/60032 (64%)]\tLoss: 0.013651\n",
      "Train Epoch: 9 [40320/60032 (67%)]\tLoss: 0.013208\n",
      "Train Epoch: 9 [42240/60032 (70%)]\tLoss: 0.026848\n",
      "Train Epoch: 9 [44160/60032 (74%)]\tLoss: 0.045252\n",
      "Train Epoch: 9 [46080/60032 (77%)]\tLoss: 0.055210\n",
      "Train Epoch: 9 [48000/60032 (80%)]\tLoss: 0.007187\n",
      "Train Epoch: 9 [49920/60032 (83%)]\tLoss: 0.095802\n",
      "Train Epoch: 9 [51840/60032 (86%)]\tLoss: 0.008833\n",
      "Train Epoch: 9 [53760/60032 (90%)]\tLoss: 0.065421\n",
      "Train Epoch: 9 [55680/60032 (93%)]\tLoss: 0.037074\n",
      "Train Epoch: 9 [57600/60032 (96%)]\tLoss: 0.007195\n",
      "Train Epoch: 9 [59520/60032 (99%)]\tLoss: 0.016787\n",
      "\n",
      "Test set: Average loss: 0.0491, Accuracy: 9844/10000 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/60032 (0%)]\tLoss: 0.033384\n",
      "Train Epoch: 10 [1920/60032 (3%)]\tLoss: 0.059348\n",
      "Train Epoch: 10 [3840/60032 (6%)]\tLoss: 0.006689\n",
      "Train Epoch: 10 [5760/60032 (10%)]\tLoss: 0.017567\n",
      "Train Epoch: 10 [7680/60032 (13%)]\tLoss: 0.005945\n",
      "Train Epoch: 10 [9600/60032 (16%)]\tLoss: 0.013802\n",
      "Train Epoch: 10 [11520/60032 (19%)]\tLoss: 0.082272\n",
      "Train Epoch: 10 [13440/60032 (22%)]\tLoss: 0.036255\n",
      "Train Epoch: 10 [15360/60032 (26%)]\tLoss: 0.037167\n",
      "Train Epoch: 10 [17280/60032 (29%)]\tLoss: 0.137820\n",
      "Train Epoch: 10 [19200/60032 (32%)]\tLoss: 0.012390\n",
      "Train Epoch: 10 [21120/60032 (35%)]\tLoss: 0.002350\n",
      "Train Epoch: 10 [23040/60032 (38%)]\tLoss: 0.009676\n",
      "Train Epoch: 10 [24960/60032 (42%)]\tLoss: 0.114615\n",
      "Train Epoch: 10 [26880/60032 (45%)]\tLoss: 0.013871\n",
      "Train Epoch: 10 [28800/60032 (48%)]\tLoss: 0.026271\n",
      "Train Epoch: 10 [30720/60032 (51%)]\tLoss: 0.004499\n",
      "Train Epoch: 10 [32640/60032 (54%)]\tLoss: 0.036721\n",
      "Train Epoch: 10 [34560/60032 (58%)]\tLoss: 0.020608\n",
      "Train Epoch: 10 [36480/60032 (61%)]\tLoss: 0.073311\n",
      "Train Epoch: 10 [38400/60032 (64%)]\tLoss: 0.116034\n",
      "Train Epoch: 10 [40320/60032 (67%)]\tLoss: 0.024484\n",
      "Train Epoch: 10 [42240/60032 (70%)]\tLoss: 0.031045\n",
      "Train Epoch: 10 [44160/60032 (74%)]\tLoss: 0.037432\n",
      "Train Epoch: 10 [46080/60032 (77%)]\tLoss: 0.008964\n",
      "Train Epoch: 10 [48000/60032 (80%)]\tLoss: 0.030843\n",
      "Train Epoch: 10 [49920/60032 (83%)]\tLoss: 0.004544\n",
      "Train Epoch: 10 [51840/60032 (86%)]\tLoss: 0.003863\n",
      "Train Epoch: 10 [53760/60032 (90%)]\tLoss: 0.021661\n",
      "Train Epoch: 10 [55680/60032 (93%)]\tLoss: 0.046169\n",
      "Train Epoch: 10 [57600/60032 (96%)]\tLoss: 0.028163\n",
      "Train Epoch: 10 [59520/60032 (99%)]\tLoss: 0.081758\n",
      "\n",
      "Test set: Average loss: 0.0374, Accuracy: 9872/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
    "    test(args, model, device, test_loader)\n",
    "\n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, there are dozen of improvements we could think of. We would like the computation to operate in parallel on the workers and to perform federated averaging, to update the central model every n batches only, to reduce the number of messages we use to communicate between workers, etc. These are features we're working on to make Federated Learning ready for a production environment and we'll write about them as soon as they are released!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
